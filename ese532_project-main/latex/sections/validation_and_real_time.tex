\documentclass[../main.tex]{subfiles}
\graphicspath{{\subfix{../images/}}}

\begin{document}
\begin{enumerate}
    \item[a)] Design Validation: \\
    At the beginning we focused on developing functionality for each of the different stages in the compression pipeline which included CDC, SHA, DEDUP and LZW. To enable seamless integration, during this stage we identified the way in which interfaces needed to be designed for each of the pieces to come together. This meant coming up with a high-level design of the complete control flow of our application, i.e., what data is passed to what stage and how it will process it before proceeding to the next. \\ \\
    Following is a detailed description of the design and validation that we performed for each of the aforementioned stages.
    \begin{itemize}
        \item \textbf{CDC:} Initially we started off by using our validated implementation from HW2 that used the rolling hash. This helped us make sure that we were working with a functional CDC and that it would not break the application when integrating it with the other components. As a part of our validation process we made sure that our CDC produced the same output as the python code in HW2 for varying modulus values as a part of further validation process. This was done by printing out chunk indices and redirecting them into a file, and the files created from our implementation and the python code were compared using the diff tool, to validate the functionality. \\ \\
        We did this at every stage whenever the CDC was optimized as a sanity check and also to preemptively catch any bugs that would arise in the future. Apart from using text files we also used binary data to validate the functionality. \\

        \item \textbf{SHA:} From the beginning we focused on getting a fully functional SHA implementation since it was a critical part of the whole application. The output from SHA would decide if our chunk was a duplicate and whether it needed to be compressed, and if our SHA produced the same digest for different chunks it would severely harm the overall functionality. \\ \\
        Our implementation for SHA-256 was based on the \href{https://github.com/james-ben/mpsoc-crypto}{mpsoc-crypto} library, which contained both the software and NEON versions of SHA-256. Initially aiming for functionality we went with implementing the software version. This library had its own test test files that we first used to make sure that the SHA implementation was valid and legitimate. Once that was done we integrated that library into our implementation. Once that was done we tested it with our interface to validate that the functionality was preserved. The testing was done in two parts. Firstly we used the linux command line utility called sha256sum as our gold standard, and compared its output against it. Following that to further validate our design we used the NIST examples to test out the SHA implementation, provided \href{https://csrc.nist.gov/CSRC/media/Projects/Cryptographic-Standards-and-Guidelines/documents/examples/SHA256.pdf}{here}. We tested against the inputs and compared the generated digest with the digest provided in the reference.\\
        We made sure that our design was validated both on text and binary data to ensure full functionality. \\

        \item \textbf{DEDUP:}  Our implementation for DEDUP used unordered\_maps to lookup SHA fingerprints and check if an entry for it already existed, if it did we would return the value, i.e., the chunk index mapped to that fingerprint or return -1 after inserting the digest into the table with a new chunk index. \\ \\
        To validate this we made sure that previously seen SHA fingerp  rints were producing the same chunk index, upon lookup into the map. This was done by testing the output from the same SHA fingerprint and validating that both the calls to dedup returned the same value. Apart from this we also tested our design by providing unique SHA fingerprints and checking the return value from the function (which should be -1, after the new mapping from SHA to chunk id is inserted into the map). Finally we tested if our design would produce the right chunk index after providing a new SHA fingerprint and then calling dedup again to check if the new fingerprint was inserted into the map with the right chunk index. \\

        \item \textbf{LZW:}  For this part we started off with the LZW implementation from geeks for geeks and modified it so that it did not use any unsupported C++ constructs like strings and vectors. We made sure that all our parts could be easily ported to the FPGA if they were to turn out to be the bottleneck. \\ \\
        Our interface for LZW required modifying the function to accept start and end indices into a big buffer, which represented the “chunk”. Hence, we needed to make sure that the implementation worked for the chunk being somewhere in between the buffer, at the beginning of the buffer and at the end of the buffer. This was done by randomly generating start and end indices and passing them as arguments to the LZW function. This also helped us validate our loop bounds and ensured we were processing all the data that was passed into the function.\\ \\
        Initially our validation process for this stage consisted of comparing the output from our implementation to the one produced by the geeks for geeks version. We started out by testing small strings like “I am Sam, Sam I am, am I Sam” and made sure that our output produced the same results as compared with the original. We then moved onto LittlePrince and finally moved onto small binary files. Once we were sure that our implementation was functioning correctly, we tested with larger text files like Franklin and the \href{https://github.com/wess/iotr/blob/master/lotr.txt}{Lord of the Rings}, and after this passed. After this we moved to testing large files like a zip archive for a font named FiraCode (20MB) and tar file for a graphical editor named neovide (24 MB).
    \end{itemize}

Now for the application as a whole once we had integrated everything we started out with a simple packet emulation program, that would read blocksize amount of bytes from a file at a time and copy them into a 16KB buffer in a loop. Once this buffer was filled up we would call our compression\_pipeline function to process that buffer. This helped us quickly weed out any problems that occurred during the integration phase since we were able to quickly test out our design on the PC itself instead of migrating all the binary files to the board and testing it there. It also aided in simplifying the original implementation so that we could focus on squashing any bugs in the standalone implementations of CDC, SHA, DEDUP and LZW. \\ \\
After the packet emulation was fully functional we moved onto using the data packets transmitted via ethernet and running the whole application on the board (everything was mapped to a single ARM processor at this point). This helped us validate our design with all the intricacies involved in packet transfer. Apart from that this phase aided in identifying the bottleneck since we used stopwatch class to time and profile the different stages of our application. This data backed our decision of mapping LZW onto the FPGA. \\ \\
Finally, when LZW was to be mapped to the FPGA we needed to focus on verifying its hardware implementation. This is when we wrote our testbench in Vitis that would compare the output from our kernel with the golden implementation from geeks for geeks. Initially, the testbench would read chunk\_size data into a buffer and pass that to the LZW kernel for processing. \\ \\
This worked for our initial implementations but when we optimized our host code to just call LZW once and process a batch of chunks we realized that to get a good idea of the functionality we had to expand the scope of testing, and emulate the conditions that the kernel is actually going to be running in, i.e., receiving start and end indices to chunks in a large buffer. So, we created an array that would hold randomly generated chunk indices and tested the kernel with them. Moving forward we decided that to emulate and generate realistic chunk indices we needed to use our CDC implementation and hence we included a function call to CDC in our testbench. \\ \\
Finally, based on some bugs that we faced (which are outlined in the section below) we decided to enhance our testing routine. We would now read in the whole file 16KB at a time in a loop, pass that buffer through CDC, generate chunk indices, and then pass that data to our kernel for testing. Following which the output from the kernel would be compared against the golden implementation at the granularity of a chunk. The packet length and the actual data was compared against the golden implementation to make sure that our kernel was not producing any incorrect values. This helped us understand the level of current functionality and the points at which our implementation would break, and in this way we could fix our design before deploying it onto the FPGA.

\item[b)] Real-Time Guarantee:\\
Based on our extensive testing and validation we can say with confidence that our design is completely functional and stable with a throughput of approximately 70 Mb/s as per the “linux.tar” test for files up to 200 MB with an -s parameter of 2000 at a block size of 8192B. The -s parameter is only required in case of large files like the “linux.tar” and the “FiraCode” archive (which required an -s parameter of 1000). Whereas for smaller files like “Franklin.txt” and “LittlePrince.txt” we noticed that the -s parameter was not required. We observed this trend of a smaller or no -s parameter as the size of the file reduced. As per our reasoning this may be due to the fact that our CDC creates chunks based on the input data, and the number of chunks may vary depending on the content. As the number of chunks increase the processing time for each 16KB data buffer that we maintain increases, and this may change dynamically based on the type of the input, and since the probability of having this worst case delay goes up with a higher number of packets (for large files), the -s parameter also needs to go up. \\

    Commands to run our design:
    \begin{itemize}
        \item \textbf{On a Mac (Host):}
            Run the following command on your board (by default the file name for the compressed file is \textit{compressed\_file.bin},
            \begin{verbatim}
$ ./encoder -k encoder.xclbin -f <Filename>
            \end{verbatim}
            Run the following command in your host computer,
            \begin{verbatim}
$ ./client_mac -i <board_ip_address> -b 8192 -f <Filename> -s 1000
            \end{verbatim}
            Now to decode the file run,
            \begin{verbatim}
$ ./decoder <compressedFilename> <outputFilename>
            \end{verbatim}

        \item \textbf{On a Windows (Host):}
            Run the following command on your board (by default the file name for the compressed file is \textit{compressed\_file.bin},
            \begin{verbatim}
$ ./encoder -k encoder.xclbin -f <Filename>
            \end{verbatim}
            Run the following command in your host computer,
            \begin{verbatim}
$ ./client -i <board_ip_address> -b 1024 -f <Filename> -s 1000
            \end{verbatim}
            Now to decode the file run,
            \begin{verbatim}
$ ./decoder <compressedFilename> <outputFilename>
            \end{verbatim}
    \end{itemize}

\item[c)] Challenges and Debugging:\\
During the development of this project, we faced a lot of challenges and bugs which were uncovered during the thorough testing that we carried out for each of the different stages. Following is a detailed description of all the bugs that we fixed and the challenges that we faced broken down based on the different stages of the data pipeline.
\begin{itemize}
    \item \textbf{CDC:} When we were integrating our CDC into the whole application, we noticed that CDC was creating chunk boundaries at every character. After carefully analyzing the code and stepping through the code in GDB we noticed that the offset that we used for filling up the 16KB buffer was invalid and this resulted in CDC operating on garbage data. Once this was identified we fixed our offset calculation to use the length from the header of the ethernet packet and this rectified our issue. \\
    
    \item \textbf{SHA/DEDUP:} In this stage we faced more of a challenge than a bug, which was related to storing the SHA fingerprints as keys into an unordered\_map that was used to map SHA digests to chunk indices. Now, the unordered\_map in C++ can’t efficiently map keys which are represented as containers like vectors, arrays, etc. This meant that we couldn’t store our SHA digest as a byte array but had to convert it into a form that could be represented by a single data type, which was a string. This conversion of the byte array to a string proved to be really challenging since we had to take special care while converting “0”s presents in the SHA fingerprint since NULL (0x00) characters are considered to be string terminators in C. To mitigate this problem we used the stringstream class in C++. Our solution included creating a hexadecimal stringstream and iterating over the byte array. At each iteration we would cast the current byte into an int using the static\_cast directive, and append the int value as a two character wide hexadecimal string to the output string. Finally after every byte was processed we would convert the stringstream object into a string and return that from SHA so that DEDUP could then utilize an unordered\_map. \\
    
    Apart from that in the SHA library we noticed that the implementation used strlen to find the length of the string and compute the SHA based on that, but we knew that this wouldn’t work on binary data since as described earlier NULL (0x00) characters are considered to be string terminators in C, and this would result in the string getting cut out prematurely which in-turn would give out an incorrect SHA. To pre-emptively catch this we adapted the library to use start and end indices into a buffer representing the chunk to ensure that every chunk is processing completely. \\

    \item \textbf{LZW:} The first bug that we faced in LZW was related to the last character not showing up properly in the decoded output for a chunk. This happened in all the files that we tested, and it pointed us to our incorrect handling of the loop bounds for the last character. When we analyzed the code using gdb we found out that the loop terminated prematurely because our loop bound was set by subtracting 1 from the end index whereas the input passed to LZW already took that into account and this led to an off by one error. Once it was identified we fixed it by removing the subtraction by 1. \\
    
    After we had moved to the FPGA mapping of the LZW we noticed that the implementation was failing when working with binary data. This was due to the fact that the first 256 entries in the hash table were initialized with the key being the ASCII values of all the characters followed by a zero to the respective ASCII values of the character itself, i.e., ‘a0’ was mapped to 97 since the ASCII value of ‘a’ is 97. This meant that a sequence of characters represented by ‘00’ was already mapped to a value of 0, although actually we haven’t seen that mapping before and hence the mapping should really be of an invalid character followed by a valid character to ensure that we aren’t mapping any sequences accidentally. To ensure this we can just remove the initialization of the hash table and keep the next code set to 256. This way we ensure that there are no invalid mapping present in the hash table which will prevent from getting any false positive lookups. \\
    
    Another problem that we faced while processing binary data with LZW was that our implementation would fail at random places within the binary file and work perfectly fine most of the time. It took us a lot of time trying to understand where the problem was occurring, and since we were using Vitis there wasn’t a good debugging tool available. So, firstly we included some debugging print statements in the code to find the point of failure (this included the iteration index, offset, etc.). After that we decided to compile the testbench and our kernel using g++ outside vitis and used two gdb instances, to stop at the exact iteration and offset in both our kernel and the golden implementation parallelly. There we first examined the prefix and the next character to ensure that the problem didn’t occur in a previous iteration. We found that both of them were the same, then we advanced the code in the golden implementation to check if the sequence was present in the map and was found or was it inserted into the map this iteration, and surprisingly our implementation was also following the same pattern. This suggested that there was a problem with the value returned by the lookup. We checked the current code value and found that the value was exceeding our set maximum of 4096, and due to this our hash lookup returned a value that wasn’t associated with that particular key, since our hash table was configured to have a key consisting of a 12-bit value and an 8-bit character. Due to this our complete implementation broke down and we were getting random points of failure. \\
    
    Once we found this out, we decided that we will need to move to 13-bit hash table configuration and 13-bit packed output since our lzw codes were exceeding the value represented by 12-bits. Hence, we modified the code accordingly. After that we encountered a similar problem. We were still getting random failures. After setting up the parallel gdb instances again we found that the problem was with overflow, the insert function in the associative memory didn’t set the proper bit in memory when the fill value exceeded 32, since $1 << 32$ exceeded the int maximum value. So, after identifying that issue, we modified the code to use 1UL instead of 1 to increase the length of the data type and prevent overflow, and the bug was fixed. 
\end{itemize}


\end{enumerate}
\end{document}